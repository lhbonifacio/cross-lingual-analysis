{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translate_ASSIN2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq-HnrSvlqFp"
      },
      "source": [
        "!pip install torchtext==0.8.0\n",
        "!pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\n",
        "!pip install ftfy --quiet\n",
        "!pip install transformers --quiet \n",
        "!pip install sentencepiece --quiet\n",
        "!git clone https://github.com/unicamp-dl/cross-lingual-analysis.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pua18Ngoo0Aq"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from ftfy import fix_encoding\n",
        "import ftfy\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev, torch.cuda.get_device_name(0))\n",
        "device = torch.device(dev)\n",
        "\n",
        "# Model\n",
        "model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
        "marian_tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "marian_model = MarianMTModel.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CUBXXNXsiXw"
      },
      "source": [
        "nlp = English()\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "def chunkstring_spacy(text):\n",
        "    \"\"\"\n",
        "    Segment text and prepare to translation\n",
        "\n",
        "    Args:\n",
        "      text: Sentence to be translated\n",
        "      \n",
        "    Returns:\n",
        "      Segmented text.\n",
        "    \"\"\"\n",
        "    chunck_sentences = []\n",
        "    doc = nlp(str(text))\n",
        "    for sent in doc.sents:\n",
        "        chunck_sentences.append('>>en<<' + ' ' + sent.text)\n",
        "        \n",
        "    return chunck_sentences\n",
        "\n",
        "def translate(aux_sent):\n",
        "    \"\"\"\n",
        "    Translate text\n",
        "\n",
        "    Args:\n",
        "      aux_sent: Sentence to be translated\n",
        "      \n",
        "    Returns:\n",
        "      Translated text.\n",
        "    \"\"\"\n",
        "    max_length = 512\n",
        "    num_beams = 1\n",
        "\n",
        "    sentence = chunkstring_spacy(aux_sent)\n",
        "\n",
        "    #Move o modelo para a GPU\n",
        "    marian_model.to(device)\n",
        "    marian_model.eval()\n",
        "\n",
        "    tokenized_text = marian_tokenizer.prepare_seq2seq_batch(sentence, max_length=max_length, return_tensors='pt')\n",
        "                        \n",
        "    translated = marian_model.generate(input_ids=tokenized_text['input_ids'].to(device), \n",
        "                                        max_length=max_length, \n",
        "                                        num_beams=num_beams, \n",
        "                                        early_stopping=True, \n",
        "                                        do_sample=False)\n",
        "                        \n",
        "    tgt_text = [fix_encoding(marian_tokenizer.decode(t, skip_special_tokens=True)) for t in translated]\n",
        "    return ' '.join(tgt_text)\n",
        "\n",
        "def translate_assin(input):\n",
        "    \"\"\"\n",
        "    Translate ASSIN2 test set to English\n",
        "    Args:\n",
        "      input: Dataset to be translated\n",
        "\n",
        "    Returns:\n",
        "      CSV containing the translated dataset.\n",
        "    \"\"\"\n",
        "    print('Translating ASSIN2 ...')\n",
        "    df = pd.read_csv('{}'.format(input), encoding='utf-8')\n",
        "\n",
        "    lista = df['t'].tolist()\n",
        "    lista2 = df['h'].tolist()\n",
        "    label = df['_entailment'].tolist()\n",
        "    id = df['_id'].tolist()\n",
        "    similarity = df['_similarity'].tolist()\n",
        "\n",
        "    list_sent_1 = []\n",
        "    list_sent_2 = []\n",
        "    list_label = []\n",
        "    list_id = []\n",
        "    list_sim = []\n",
        "    for sent1, sent2, l, i, s in zip(tqdm(lista), lista2, label, id, similarity): \n",
        "        sent = str(sent1) + '. ' + str(sent2)\n",
        "        saida_sent = translate(sent)\n",
        "        new_saida_sent = saida_sent.replace('.','').replace('-','')\n",
        "  \n",
        "        try:\n",
        "            new_sent1 = new_saida_sent.split('  ')[0]\n",
        "            new_sent2 = new_saida_sent.split('  ')[1]\n",
        "            list_sent_1.append(new_sent1)\n",
        "            list_sent_2.append(new_sent2)\n",
        "            list_label.append(l)\n",
        "            list_id.append(i)\n",
        "            list_sim.append(s)\n",
        "        except:  \n",
        "            pass      \n",
        "\n",
        "    df_final = pd.DataFrame(columns = df.columns)\n",
        "    df_final['t'] = list_sent_1\n",
        "    df_final['h'] = list_sent_2\n",
        "    df_final['_entailment'] = list_label\n",
        "    df_final['_id'] = list_id\n",
        "    df_final['_similarity'] = list_sim\n",
        "\n",
        "    df_final.to_csv('/content/assin2-translated.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heSXPBZ-vZtv"
      },
      "source": [
        "### Run code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV3y5JhS0K9O"
      },
      "source": [
        "input = '/content/cross-lingual-analysis/data/assin2-test.csv'\n",
        "translate_assin(input)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}