{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translate_SQUAD.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nng39FE13Y5c"
      },
      "source": [
        "!pip install torchtext==0.8.0\n",
        "!pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\n",
        "!pip install ftfy --quiet\n",
        "!pip install transformers --quiet \n",
        "!pip install sentencepiece --quiet\n",
        "\n",
        "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
        "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TigOLwgh3tfK"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from ftfy import fix_encoding\n",
        "import ftfy\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev, torch.cuda.get_device_name(0))\n",
        "device = torch.device(dev)\n",
        "\n",
        "# Model\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
        "marian_tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "marian_model = MarianMTModel.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ilJTe1z4X4U"
      },
      "source": [
        "nlp = English()\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "def chunkstring_spacy(text):\n",
        "    \"\"\"\n",
        "    Segment text and prepare to translation\n",
        "\n",
        "    Args:\n",
        "      text: Sentence to be translated\n",
        "      \n",
        "    Returns:\n",
        "      Segmented text.\n",
        "    \"\"\"\n",
        "    chunck_sentences = []\n",
        "    doc = nlp(str(text))\n",
        "    for sent in doc.sents:\n",
        "        chunck_sentences.append('>>pt_br<<' + ' ' + sent.text)\n",
        "        \n",
        "    return chunck_sentences\n",
        "\n",
        "def translate(aux_sent):\n",
        "    \"\"\"\n",
        "    Translate text\n",
        "\n",
        "    Args:\n",
        "      aux_sent: Sentence to be translated\n",
        "      \n",
        "    Returns:\n",
        "      Translated text.\n",
        "    \"\"\"\n",
        "    max_length = 512\n",
        "    num_beams = 1\n",
        "    sentence = chunkstring_spacy(aux_sent)\n",
        "\n",
        "    marian_model.to(device)\n",
        "    marian_model.eval()\n",
        "    tokenized_text = marian_tokenizer.prepare_seq2seq_batch(sentence, max_length=max_length) \n",
        "\n",
        "    translated = marian_model.generate(input_ids=torch.LongTensor(tokenized_text['input_ids']).to(device), \n",
        "                                        max_length=max_length, \n",
        "                                        num_beams=num_beams, \n",
        "                                        early_stopping=True, \n",
        "                                        do_sample=False)                        \n",
        "    tgt_text = [fix_encoding(marian_tokenizer.decode(t, skip_special_tokens=True)) for t in translated]\n",
        "    return ' '.join(tgt_text)\n",
        "\n",
        "def insert_dash(string, index, mode):\n",
        "    \"\"\"\n",
        "    Insert special tokens between answer span\n",
        "\n",
        "    Args:\n",
        "      string: Dataset context\n",
        "      index: Position in context\n",
        "      mode: At the beginning or at the end of the answer\n",
        "    Returns:\n",
        "      Dataset context with special tokens\n",
        "    \"\"\"\n",
        "    if mode == True:\n",
        "        return string[:index] + ' ###### ' + string[index:]  \n",
        "    else:\n",
        "        return string[:index] + ' $$$ ' + string[index:]\n",
        "\n",
        "def get_answer_2(tgt_text):\n",
        "    \"\"\"\n",
        "    Get answer span and its index\n",
        "\n",
        "    Args:\n",
        "      tgt_text: Context\n",
        "      \n",
        "    Returns:\n",
        "      Answer span and start answer index\n",
        "    \"\"\"\n",
        "    x = [c for c in tgt_text.split() if c.startswith('##')]\n",
        "    idx = str(tgt_text).find(x[0]) +len(x[0]) + 1 \n",
        "\n",
        "    inicio = tgt_text.index(x[0])\n",
        "    y = [d for d in tgt_text.split() if d.startswith('$$')]\n",
        "    fim = tgt_text.index(y[0])\n",
        "    answer = ''.join(tgt_text[inicio+1+len(x[0]):fim])\n",
        "\n",
        "    return answer, idx \n",
        "\n",
        "def remove_token(tgt_text):\n",
        "    \"\"\"\n",
        "    Remove special tokens in the context\n",
        "    Args:\n",
        "      tgt_text: Context\n",
        "      \n",
        "    Returns:\n",
        "      Context without special tokens\n",
        "    \"\"\"\n",
        "    context_used = [c for c in tgt_text.split() if not c.startswith('##')]\n",
        "    context = [d for d in context_used if not d.startswith('$$')]\n",
        "    context = ' '.join(context) \n",
        "    return context\n",
        "\n",
        "def translate_squad(input):\n",
        "    \"\"\"\n",
        "    Translate SQuAD train set to Portuguese\n",
        "    Args:\n",
        "      input: Dataset to be translated\n",
        "\n",
        "    Returns:\n",
        "      Json containing the translated dataset.\n",
        "    \"\"\"\n",
        "    input_file = input\n",
        "    print('Translating SQuAD ...')\n",
        "    with open(input_file) as f: \n",
        "        document = json.load(f) \n",
        "        dict_3 = {}\n",
        "        list_2 = []\n",
        "        for k in tqdm(range(len(document['data']))):  \n",
        "            list_1 = []\n",
        "            dict_2 = {}\n",
        "            for i in range (len(document['data'][k]['paragraphs'])):         \n",
        "                final_sent = document['data'][k]['paragraphs'][i]['context']   \n",
        "                dict_1 = {}           \n",
        "                list_0 = [] \n",
        "                for j in range (len(document['data'][k]['paragraphs'][i]['qas'])):\n",
        "                        dict_0 = {}    \n",
        "                        question = document['data'][k]['paragraphs'][i]['qas'][j]['question']\n",
        "                        ans = document['data'][k]['paragraphs'][i]['qas'][j]['answers'][0]['text']\n",
        "                        ans_len = len(document['data'][k]['paragraphs'][i]['qas'][j]['answers'][0]['text']) \n",
        "                        start_token = document['data'][k]['paragraphs'][i]['qas'][j]['answers'][0]['answer_start']\n",
        "                        end_token = start_token + (ans_len+7)            \n",
        "                        aux_sent = insert_dash(final_sent, start_token, True)\n",
        "                        aux_sent = insert_dash(aux_sent, end_token, False).replace('  ',' ')                     \n",
        "                        tgt_text = translate(aux_sent)\n",
        "                        tgt_question = translate(question)\n",
        "                        try:\n",
        "                            answer, answer_start = get_answer_2(tgt_text)\n",
        "                            if len(answer) == 0:\n",
        "                                pass\n",
        "                            else:\n",
        "                                id = document['data'][k]['paragraphs'][i]['qas'][j]['id']                        \n",
        "                                dict_0 = {'answers': [{'answer_start': answer_start, 'text': answer}], 'question': tgt_question, 'id': id}\n",
        "                                context_used = remove_token(tgt_text)\n",
        "                                dict_1 = {'context': context_used, 'qas': [dict_0]}     \n",
        "                                list_1.append(dict_1) \n",
        "                        except:\n",
        "                            pass   \n",
        "        \n",
        "            dict_2 = {'title':document['data'][k]['title'] , 'paragraphs': list_1 }   \n",
        "            list_2.append(dict_2)\n",
        "        dict_3 = {'data':list_2, 'version': 1.1}\n",
        "        with open('/content/squad_translated.json', 'w') as f:\n",
        "            json.dump(dict_3, f)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uznFlKFJ7nXC"
      },
      "source": [
        "input = '/content/train-v1.1.json'\n",
        "translate_squad(input)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP6WLqnGN4Qw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}